{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from random import choices\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "cudnn.benchmark = True\n",
    "Image.MAX_IMAGE_PIXELS = None  # Disable DecompressionBombError\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # Disable OSError: image file is truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TO DOS</h2>\n",
    "<ul>\n",
    "  <li>For every batch of style, take k batch of contents (as_completed()?)..to save IO time by caching style</li>\n",
    "  <li>Just train decoder with COCO for content reconstruction</li>\n",
    "  <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"d:/Logs\"\n",
    "CONTENT_DIR = \"d:/Images/Content/\"\n",
    "STYLE_DIR = \"d:/Images/Style/\"\n",
    "MAX_Iter = 100*1000\n",
    "device= torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "def InfiniteSampler(n):\n",
    "    # i = 0\n",
    "    i = n - 1\n",
    "    order = np.random.permutation(n)\n",
    "    while True:\n",
    "        yield order[i]\n",
    "        i += 1\n",
    "        if i >= n:\n",
    "            np.random.seed()\n",
    "            order = np.random.permutation(n)\n",
    "            i = 0\n",
    "\n",
    "\n",
    "class InfiniteSamplerWrapper(data.sampler.Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        self.num_samples = len(data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(InfiniteSampler(self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 ** 31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function.py\n",
    "import torch\n",
    "\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    #print(\"feat_var\",feat_var.size())\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    #print(\"feat_std\",feat_std.size())\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "\n",
    "def adaptive_instance_normalization(content_feat, style_feat):\n",
    "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
    "    size = content_feat.size()\n",
    "    style_mean, style_std = calc_mean_std(style_feat)\n",
    "    content_mean, content_std = calc_mean_std(content_feat)\n",
    "\n",
    "    normalized_feat = (content_feat - content_mean.expand(\n",
    "        size)) / content_std.expand(size)\n",
    "    #print(\"cont norm size:\",normalized_feat.size())\n",
    "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
    "\n",
    "\n",
    "def _calc_feat_flatten_mean_std(feat):\n",
    "    # takes 3D feat (C, H, W), return mean and std of array within channels\n",
    "    assert (feat.size()[0] == 3)\n",
    "    assert (isinstance(feat, torch.FloatTensor))\n",
    "    feat_flatten = feat.view(3, -1)\n",
    "    mean = feat_flatten.mean(dim=-1, keepdim=True)\n",
    "    std = feat_flatten.std(dim=-1, keepdim=True)\n",
    "    return feat_flatten, mean, std\n",
    "\n",
    "\n",
    "def _mat_sqrt(x):\n",
    "    U, D, V = torch.svd(x)\n",
    "    return torch.mm(torch.mm(U, D.pow(0.5).diag()), V.t())\n",
    "\n",
    "\n",
    "def coral(source, target):\n",
    "    # assume both source and target are 3D array (C, H, W)\n",
    "    # Note: flatten -> f\n",
    "\n",
    "    source_f, source_f_mean, source_f_std = _calc_feat_flatten_mean_std(source)\n",
    "    source_f_norm = (source_f - source_f_mean.expand_as(\n",
    "        source_f)) / source_f_std.expand_as(source_f)\n",
    "    source_f_cov_eye = \\\n",
    "        torch.mm(source_f_norm, source_f_norm.t()) + torch.eye(3)\n",
    "\n",
    "    target_f, target_f_mean, target_f_std = _calc_feat_flatten_mean_std(target)\n",
    "    target_f_norm = (target_f - target_f_mean.expand_as(\n",
    "        target_f)) / target_f_std.expand_as(target_f)\n",
    "    target_f_cov_eye = \\\n",
    "        torch.mm(target_f_norm, target_f_norm.t()) + torch.eye(3)\n",
    "\n",
    "    source_f_norm_transfer = torch.mm(\n",
    "        _mat_sqrt(target_f_cov_eye),\n",
    "        torch.mm(torch.inverse(_mat_sqrt(source_f_cov_eye)),\n",
    "                 source_f_norm)\n",
    "    )\n",
    "\n",
    "    source_f_transfer = source_f_norm_transfer * \\\n",
    "                        target_f_std.expand_as(source_f_norm) + \\\n",
    "                        target_f_mean.expand_as(source_f_norm)\n",
    "\n",
    "    return source_f_transfer.view(source.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Net.py\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 3, (3, 3)),\n",
    ")\n",
    "\n",
    "vgg = nn.Sequential(\n",
    "    nn.Conv2d(3, 3, (1, 1)),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(3, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),  # relu1-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),  # relu2-2\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),  # relu3-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu4-4\n",
    "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-1\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-2\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU(),  # relu5-3\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 512, (3, 3)),\n",
    "    nn.ReLU()  # relu5-4\n",
    ")\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Net, self).__init__()\n",
    "        enc_layers = list(encoder.children())\n",
    "        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n",
    "        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
    "        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
    "        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
    "        self.decoder = decoder\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        # fix the encoder\n",
    "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n",
    "            for param in getattr(self, name).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n",
    "    def encode_with_intermediate(self, input):\n",
    "        results = [input]\n",
    "        for i in range(4):\n",
    "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
    "            results.append(func(results[-1]))\n",
    "        return results[1:]\n",
    "\n",
    "    # extract relu4_1 from input image\n",
    "    def encode(self, input):\n",
    "        for i in range(4):\n",
    "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
    "        return input\n",
    "\n",
    "    def calc_content_loss(self, input, target):\n",
    "        assert (input.size() == target.size())\n",
    "        assert (target.requires_grad is False)\n",
    "        return self.mse_loss(input, target)\n",
    "\n",
    "    def calc_style_loss(self, input, target):\n",
    "        assert (input.size() == target.size())\n",
    "        assert (target.requires_grad is False)\n",
    "        input_mean, input_std = calc_mean_std(input)\n",
    "        target_mean, target_std = calc_mean_std(target)\n",
    "        return self.mse_loss(input_mean, target_mean) + \\\n",
    "               self.mse_loss(input_std, target_std)\n",
    "\n",
    "    def forward(self, content, style, alpha=1.0):\n",
    "        assert 0 <= alpha <= 1\n",
    "        style_feats = self.encode_with_intermediate(style)\n",
    "        content_feat = self.encode(content)\n",
    "        t = adaptive_instance_normalization(content_feat, style_feats[-1])\n",
    "        t = alpha * t + (1 - alpha) * content_feat\n",
    "\n",
    "        g_t = self.decoder(t)\n",
    "        g_t_feats = self.encode_with_intermediate(g_t)\n",
    "\n",
    "        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n",
    "        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n",
    "        for i in range(1, 4):\n",
    "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
    "        return loss_c, loss_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform():\n",
    "    transform_list = [\n",
    "        transforms.Resize(size=(512, 512)),\n",
    "        transforms.RandomCrop(256),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def test_transform(size, crop):\n",
    "    transform_list = []\n",
    "    if size != 0:\n",
    "        transform_list.append(transforms.Resize(size))\n",
    "    if crop:\n",
    "        transform_list.append(transforms.CenterCrop(size))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "class FlatFolderDataset(data.Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        super(FlatFolderDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.paths = os.listdir(self.root)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def name(self):\n",
    "        return 'FlatFolderDataset'\n",
    "\n",
    "class FlataDataset(data.Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        super(FlataDataset, self).__init__()\n",
    "        self.gen = os.walk(root)\n",
    "        next(self.gen)\n",
    "        self.paths = []\n",
    "        for a,b,c in self.gen:\n",
    "            for p in c:\n",
    "                self.paths.append(os.path.join(a,p))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def name(self):\n",
    "        return 'FlatFolderDataset'\n",
    "\n",
    "def adjust_learning_rate(optimizer, iteration_count):\n",
    "    lr = 1e-4 / (1.0 + 5e-5 * iteration_count)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer(vgg, decoder, content, style, alpha=.7,\n",
    "                   interpolation_weights=None):\n",
    "    assert (0.0 <= alpha <= 1.0)\n",
    "    x = clock()\n",
    "    content_f = vgg(content)\n",
    "    style_f = vgg(style)\n",
    "    y = clock()\n",
    "    feat = adaptive_instance_normalization(content_f, style_f)\n",
    "    feat = feat * alpha + content_f * (1 - alpha)\n",
    "    fpoo = decoder(feat)\n",
    "    z = clock()\n",
    "    print(y-x,z-y)\n",
    "    return fpoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "vgg.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load(\"d:/AdaIn/models/decoder.pth\"))\n",
    "vgg.load_state_dict(torch.load(\"d:/AdaIn/models/vgg_normalised.pth\"))\n",
    "vgg = nn.Sequential(*list(vgg.children())[:31])\n",
    "\n",
    "vgg.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "content_tf = test_transform(0,False)\n",
    "style_tf = test_transform(0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.165758104737449 1.3647617538710506\n",
      "1.6094823998778054 0.022613876492641793\n",
      "1.3884184025106379 0.019696617589630705\n",
      "1.7078940923516086 0.01954870664678765\n",
      "1.708259603046855 0.06426474465877163\n",
      "2.4597567035657164 2.9483625831963423\n",
      "0.005729557925462814 0.014110135056910167\n",
      "0.005871779985881176 0.003820937874849051\n",
      "0.004838394494981912 0.0028936500409884047\n",
      "0.005681486869065111 0.0038360134132346957\n",
      "1.7808685159895958 2.0833597629329006\n",
      "0.011240947210296781 0.00935593602167728\n",
      "0.004522945964993141 0.002916121126560256\n",
      "0.004461506034914464 0.002973294394820414\n",
      "0.005823140041229635 0.0038286178661053327\n",
      "1.3153481212038969 0.6982019433791322\n",
      "0.009981713087483968 0.0029835343831905448\n",
      "0.004448990493585825 0.0029232322295627\n",
      "0.011344769314405312 0.003401951684878668\n",
      "0.004489097114628748 0.002915552238306418\n",
      "1.9842898667635325 1.264280161530138\n",
      "0.005674375766062667 0.010298299393923571\n",
      "0.006218232925050415 0.002911854464713315\n",
      "0.0047160835230215525 0.0028993389234130973\n",
      "0.004707265755286016 0.0029448499827537944\n",
      "1.0728974644366633\n"
     ]
    }
   ],
   "source": [
    "tot,it = 0,0\n",
    "filt = lambda s:s.startswith('_')\n",
    "for f in filter(filt,os.listdir(CONTENT_DIR)):\n",
    "    for g in filter(filt,os.listdir(STYLE_DIR)):\n",
    "        Ic,Is = transforms.ToTensor()(Image.open(CONTENT_DIR+f)),transforms.ToTensor()(Image.open(STYLE_DIR+g))\n",
    "        Ic,Is = Ic.to(device).unsqueeze(0),Is.to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            st = time.clock()\n",
    "            output = style_transfer(vgg, decoder,Ic,Is,alpha=.9)\n",
    "            tot += (time.clock()-st)\n",
    "            it += 1\n",
    "        save_image(output,f\"d:/AdaIN/Outputs/{f[1:5]}_{g[1:5]}.jpg\")\n",
    "        \n",
    "print(tot/it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
