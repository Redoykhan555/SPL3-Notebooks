{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random  \n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "INIT_LR = .0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.ins0 = nn.InstanceNorm2d(3,affine=True)\n",
    "        self.rpad = nn.ReflectionPad2d(15)\n",
    "        self.conv1 = nn.Conv2d(3,32,kernel_size=3,stride=1)\n",
    "        self.ins1 = nn.InstanceNorm2d(32,affine=True)\n",
    "        self.conv2 = nn.Conv2d(32,32,kernel_size=3,stride=2)\n",
    "        self.ins2 = nn.InstanceNorm2d(32,affine=True)\n",
    "        self.conv3 = nn.Conv2d(32,64,kernel_size=3,stride=2)\n",
    "        self.ins3 = nn.InstanceNorm2d(64,affine=True)\n",
    "        self.conv4 = nn.Conv2d(64,128,kernel_size=3,stride=2)\n",
    "        self.ins4 = nn.InstanceNorm2d(128,affine=True)\n",
    "        self.conv5 = nn.Conv2d(128,256,kernel_size=3,stride=2)\n",
    "        self.ins5 = nn.InstanceNorm2d(256,affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.ins0(x)\n",
    "        x = self.rpad(x)\n",
    "        x = self.relu(self.ins1(self.conv1(x)))\n",
    "        x = self.relu(self.ins2(self.conv2(x)))\n",
    "        x = self.relu(self.ins3(self.conv3(x)))\n",
    "        x = self.relu(self.ins4(self.conv4(x)))\n",
    "        x = self.relu(self.ins5(self.conv5(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.rpad = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.rpad(x)\n",
    "        out = self.relu(self.in1(self.conv1(out)))\n",
    "        out = self.in2(self.conv2(self.rpad(out)))\n",
    "        out = out + residual\n",
    "        return out\n",
    "    \n",
    "class TransforBlock(nn.Module): #different from tf implementation but same as paper\n",
    "    def __init__(self):\n",
    "        super(TransforBlock,self).__init__()\n",
    "        self.rpad = nn.ReflectionPad2d(4)\n",
    "        self.conv = nn.AvgPool2d(kernel_size=10,stride=1) #nn.Conv2d(3,1,kernel_size=10,stride=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.rpad(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.res1 = ResidualBlock(256)\n",
    "        self.res2 = ResidualBlock(256)\n",
    "        self.res3 = ResidualBlock(256)\n",
    "        self.res4 = ResidualBlock(256)\n",
    "        self.res5 = ResidualBlock(256)\n",
    "        self.res6 = ResidualBlock(256)\n",
    "        self.res7 = ResidualBlock(256)\n",
    "        self.res8 = ResidualBlock(256)\n",
    "        self.res9 = ResidualBlock(256)\n",
    "        \n",
    "        self.upconv1 = nn.Conv2d(256,256,kernel_size=3,stride=1)\n",
    "        self.ins1 = nn.InstanceNorm2d(256,affine=True)\n",
    "        self.upconv2 = nn.Conv2d(256,128,kernel_size=3,stride=1)\n",
    "        self.ins2 = nn.InstanceNorm2d(128,affine=True)\n",
    "        self.upconv3 = nn.Conv2d(128,64,kernel_size=3,stride=1)\n",
    "        self.ins3 = nn.InstanceNorm2d(64,affine=True)\n",
    "        self.upconv4 = nn.Conv2d(64,32,kernel_size=3,stride=1)\n",
    "        self.ins4 = nn.InstanceNorm2d(32,affine=True)\n",
    "        self.rpad = nn.ReflectionPad2d(3)\n",
    "        self.upconv5 = nn.Conv2d(32,3,kernel_size=7,stride=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.zpad = nn.ZeroPad2d(1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.res5(x)\n",
    "        x = self.res6(x)\n",
    "        x = self.res7(x)\n",
    "        x = self.res8(x)\n",
    "        x = self.res9(x)\n",
    "        \n",
    "        x = self.relu(self.ins1(self.upconv1(self.zpad(F.interpolate(x,scale_factor=2)))))\n",
    "        x = self.relu(self.ins2(self.upconv2(self.zpad(F.interpolate(x,scale_factor=2)))))\n",
    "        x = self.relu(self.ins3(self.upconv3(self.zpad(F.interpolate(x,scale_factor=2)))))\n",
    "        x = self.relu(self.ins4(self.upconv4(self.zpad(F.interpolate(x,scale_factor=2)))))\n",
    "        \n",
    "        x = self.rpad(x)\n",
    "        x = self.upconv5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.conv0 = nn.Conv2d(3,128,kernel_size=5,stride=2)\n",
    "        self.in0 = nn.InstanceNorm2d(128)\n",
    "        self.conv1 = nn.Conv2d(128,128,kernel_size=2,stride=2)\n",
    "        self.in1 = nn.InstanceNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128,256,kernel_size=5,stride=2)\n",
    "        self.in2 = nn.InstanceNorm2d(256)\n",
    "        self.conv3 = nn.Conv2d(256,512,kernel_size=5,stride=2)\n",
    "        self.in3 = nn.InstanceNorm2d(512)\n",
    "        self.conv4 = nn.Conv2d(512,512,kernel_size=5,stride=2)\n",
    "        self.in4 = nn.InstanceNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512,1024,kernel_size=5,stride=2)\n",
    "        self.in5 = nn.InstanceNorm2d(1024)\n",
    "        self.conv6 = nn.Conv2d(1024,1024,kernel_size=5,stride=2)\n",
    "        self.in6 = nn.InstanceNorm2d(1024)\n",
    "        \n",
    "        self.conv0_pred = nn.Conv2d(128,1,kernel_size=5,stride=1)\n",
    "        self.conv1_pred = nn.Conv2d(128,1,kernel_size=10,stride=1)\n",
    "        self.conv3_pred = nn.Conv2d(512,1,kernel_size=10,stride=1)\n",
    "        self.conv5_pred = nn.Conv2d(1024,1,kernel_size=6,stride=1)\n",
    "        self.conv6_pred = nn.Conv2d(1024,1,kernel_size=3,stride=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.in0(self.conv0(x)),negative_slope=.2)\n",
    "        p0 = self.conv0_pred(x)\n",
    "        x = F.leaky_relu(self.in1(self.conv1(x)),negative_slope=.2)\n",
    "        p1 = self.conv1_pred(x)\n",
    "        x = F.leaky_relu(self.in2(self.conv2(x)),negative_slope=.2)\n",
    "        x = F.leaky_relu(self.in3(self.conv3(x)),negative_slope=.2)\n",
    "        p3 = self.conv3_pred(x)\n",
    "        x = F.leaky_relu(self.in4(self.conv4(x)),negative_slope=.2)\n",
    "        x = F.leaky_relu(self.in5(self.conv5(x)),negative_slope=.2)\n",
    "        p5 = self.conv5_pred(x)\n",
    "        x = F.leaky_relu(self.in6(self.conv6(x)),negative_slope=.2)\n",
    "        p6 = self.conv6_pred(x)\n",
    "        return {\"scale_0\": p0,\n",
    "                \"scale_1\": p1,\n",
    "                \"scale_3\": p3, \n",
    "                \"scale_5\": p5,\n",
    "                \"scale_6\": p6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([transforms.Resize(800, Image.BICUBIC),\n",
    "                transforms.RandomCrop((768,768)),\n",
    "                transforms.ToTensor()])                \n",
    "                #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(path):\n",
    "    im = Image.open(path).convert(\"RGB\")\n",
    "    arr = transformer(im)\n",
    "    return arr.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDIR = \"d:/COCO17/val/\"\n",
    "PDIR = \"d:/A Style Aware Content Loss/data/monet_water-lilies/\"\n",
    "\n",
    "cpaths = [CDIR+f for f in os.listdir(CDIR)]\n",
    "pool = ThreadPoolExecutor(max_workers=4)\n",
    "ppaths = [PDIR+f for f in os.listdir(PDIR)][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rate = .8\n",
    "discr_succ_rate = .8\n",
    "alpha = .05\n",
    "encoder = Encoder().cuda()\n",
    "decoder = Decoder().cuda()\n",
    "discriminator = Discriminator().cuda()\n",
    "transformerBlock = TransforBlock().cuda()\n",
    "\n",
    "sceLoss = nn.BCEWithLogitsLoss()\n",
    "mseLoss = nn.MSELoss()\n",
    "absLoss = nn.L1Loss()\n",
    "Gopt = torch.optim.Adam(list(encoder.parameters())+list(decoder.parameters())+list(transformerBlock.parameters()),INIT_LR)\n",
    "Dopt = torch.optim.Adam(discriminator.parameters(),INIT_LR)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"d:/Visual/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9fec22d4564dc79a45bf6ca5c724fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dloss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2aab7a614ea3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train/Gloss\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train/Dloss\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dloss' is not defined"
     ]
    }
   ],
   "source": [
    "Glosses,Dlosses = [.5],[.5]\n",
    "for i in tqdm(range(50)):\n",
    "    photo = torch.stack(list(pool.map(take,random.choices(cpaths,k=BATCH_SIZE)))).cuda()\n",
    "    painting = torch.stack(list(pool.map(take,random.choices(ppaths,k=BATCH_SIZE)))).cuda()\n",
    "    \n",
    "    #These Tensors are common for both G & D training\n",
    "    input_photo_features = encoder(photo)\n",
    "    output_photo = decoder(input_photo_features)\n",
    "    output_photo_features = encoder(output_photo)\n",
    "    output_photo_discr_predictions = discriminator(output_photo) #a dict containing predictions at 6 different scales\n",
    "    \n",
    "    scale_weight = {\"scale_0\": 1.,\n",
    "                    \"scale_1\": 1.,\n",
    "                    \"scale_3\": 1.,\n",
    "                    \"scale_5\": 1.,\n",
    "                    \"scale_6\": 1.}\n",
    "    \n",
    "    if discr_succ_rate>=.8:\n",
    "        #-----TRAINING GENERATOR--------------------------\n",
    "        #G Losses\n",
    "        output_photo_gener_loss = {key: sceLoss(pred, torch.ones_like(pred)) * scale_weight[key]\n",
    "                                            for key, pred in output_photo_discr_predictions.items()}\n",
    "        gener_loss = sum(output_photo_gener_loss.values()) \n",
    "        img_loss = mseLoss(transformerBlock(output_photo),transformerBlock(photo))\n",
    "        feature_loss = absLoss(input_photo_features,output_photo_features)\n",
    "        \n",
    "        Gloss = gener_loss + 100*img_loss + 200*feature_loss\n",
    "        \n",
    "        Gopt.zero_grad()\n",
    "        Gloss.backward()\n",
    "        Gopt.step()\n",
    "    \n",
    "    else:\n",
    "        #-----TRAINING DISCRIMINATOR--------------------------\n",
    "        #Generate D's output for real painting & fake input photo\n",
    "        input_painting_discr_predictions = discriminator(painting)\n",
    "        input_photo_discr_predictions = discriminator(photo) \n",
    "        \n",
    "        #Compute D's loss for inp photo (fake),out photo (fake), inp painting (real)\n",
    "        \n",
    "        input_painting_discr_loss = {key: sceLoss(pred,torch.ones_like(pred))*scale_weight[key] #Do we need dict, list enough?\n",
    "                                              for key, pred in input_painting_discr_predictions.items()}\n",
    "        input_photo_discr_loss = {key: sceLoss(pred,torch.zeros_like(pred))*scale_weight[key]\n",
    "                                           for key, pred in input_photo_discr_predictions.items()}\n",
    "        output_photo_discr_loss = {key: sceLoss(pred, torch.zeros_like(pred)) * scale_weight[key]\n",
    "                                            for key, pred in output_photo_discr_predictions.items()}\n",
    "        \n",
    "        Dloss = sum(input_painting_discr_loss.values()) + sum(input_photo_discr_loss.values()) + \\\n",
    "                    sum(output_photo_discr_loss.values())\n",
    "        \n",
    "        Dopt.zero_grad()\n",
    "        Dloss.backward()\n",
    "        Dopt.step()\n",
    "        \n",
    "    if i%100==0:\n",
    "        with torch.no_grad():\n",
    "            photo = torch.stack([take(\"d:/Images/dancing.jpg\"),take(\"d:/Images/amber.jpg\")])\n",
    "            enc = encoder(photo)\n",
    "            out = decoder(enc)\n",
    "            save_image(out[0],f\"d:/outs/dancing{i}.jpg\",normalize=True)\n",
    "            save_image(out[1],f\"d:/outs/amber{i}.jpg\",normalize=True)\n",
    "    if i%10==8:\n",
    "        writer.add_scalar(\"Train/Gloss\",Gloss.item()/1000,i//2)\n",
    "        writer.add_scalar(\"Train/Dloss\",Dloss.item(),i//2)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            inp_paint_discr_acc=sum((pr.detach().cpu().numpy()>.5).mean() for pr in input_painting_discr_predictions.values())\\\n",
    "                                             /len(input_painting_discr_predictions.values())\n",
    "            inp_photo_discr_acc=sum((pr.detach().cpu().numpy()<.5).mean() for pr in input_photo_discr_predictions.values())\\\n",
    "                                             /len(input_photo_discr_predictions.values())\n",
    "            out_photo_discr_acc=sum((pr.detach().cpu().numpy()<.5).mean() for pr in output_photo_discr_predictions.values())\\\n",
    "                                             /len(output_photo_discr_predictions.values())\n",
    "            discr_succ_rate = (inp_paint_discr_acc+inp_photo_discr_acc+out_photo_discr_acc)/3\n",
    "            print(\"Discr succ:\",discr_succ_rate)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = torch.stack(list(pool.map(take,random.choices(cpaths,k=BATCH_SIZE)))).cuda()\n",
    "painting = torch.stack(list(pool.map(take,random.choices(ppaths,k=BATCH_SIZE)))).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    photo = torch.stack([take(\"d:/Images/amber.jpg\")])\n",
    "    enc = encoder(photo)\n",
    "    out = (decoder(enc)+1)/2\n",
    "    save_image(out[0],\"d:/1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_photo_features = encoder(photo)\n",
    "output_photo = decoder(input_photo_features)\n",
    "output_photo_features = encoder(output_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_painting_discr_predictions = discriminator(painting)\n",
    "input_photo_discr_predictions = discriminator(photo)  \n",
    "output_photo_discr_predictions = discriminator(output_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_weight = {\"scale_0\": 1.,\n",
    "                        \"scale_1\": 1.,\n",
    "                        \"scale_3\": 1.,\n",
    "                        \"scale_5\": 1.,\n",
    "                        \"scale_6\": 1.}\n",
    "\n",
    "input_painting_discr_loss = {key: loss(pred,torch.ones_like(pred))*scale_weight[key]\n",
    "                                              for key, pred in input_painting_discr_predictions.items()}\n",
    "input_photo_discr_loss = {key: loss(pred,torch.zeros_like(pred))*scale_weight[key]\n",
    "                                   for key, pred in input_photo_discr_predictions.items()}\n",
    "output_photo_discr_loss = {key: loss(pred, torch.zeros_like(pred)) * scale_weight[key]\n",
    "                                    for key, pred in output_photo_discr_predictions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_loss = sum(input_painting_discr_loss.values()) + sum(input_photo_discr_loss.values()) + \\\n",
    "                    sum(output_photo_discr_loss.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disc_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_photo_discr_predictions.shape,input_painting_discr_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()/(2**20),torch.cuda.memory_cached()/(2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,pred in input_painting_discr_predictions.items():\n",
    "    print(pred.shape,(pred.cpu().detach().numpy()>.5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print((pred.cpu().numpy()>.5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = lambda x:(x.mean(),x.var()**.5,x.min(),x.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = take(\"d:/Images/amber.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo(p),foo(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(img)\n",
    "b = transforms.ToTensor()(img)\n",
    "c = transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])(b)\n",
    "foo(a),foo(b),foo(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"d:/Images/amber.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
